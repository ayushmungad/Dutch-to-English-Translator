{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 1 and 2 Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGJyMHqh3OGK"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEFvxXqc3jsj"
   },
   "outputs": [],
   "source": [
    "dutch_path = \"./training_data_dutch.txt\"\n",
    "english_path = \"./training_data_english.txt\"\n",
    "result_path = \"./results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for storing and loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJMDVEpHC2J5"
   },
   "outputs": [],
   "source": [
    "def save_dict(path , word , dic):\n",
    "    pth = os.path.join(path , word + \".pickle\")\n",
    "    with open(pth , 'wb') as handle:\n",
    "        pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_dict(path , word):\n",
    "    pth = os.path.join(path , word + \".pickle\")\n",
    "    with open(pth, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = pd.read_fwf(english_path , header = None)\n",
    "df_dutch = pd.read_fwf(dutch_path , header = None)\n",
    "english_sent = df_english[0].values.tolist() #reading and storing the 1st 50000 lines of the dataset\n",
    "dutch_sent = df_dutch[0].values.tolist()\n",
    "del df_english\n",
    "del df_dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to clean one sentence, punctutations are removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogB5cXgc4eEJ"
   },
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    s =  regex.sub(\"\" , s).strip().lower()\n",
    "    arr = s.split(\" \")\n",
    "    new_arr = []\n",
    "    for w in arr:\n",
    "        if(w == \"\" or w == \" \"):\n",
    "            continue\n",
    "        else:\n",
    "            new_arr.append(w)\n",
    "    s = \" \".join(new_arr)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning all the lines in the given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfeauZoi58_z"
   },
   "outputs": [],
   "source": [
    "def clean_corpus(sent_list1 , sent_list2):\n",
    "    new_sent1 = []\n",
    "    new_sent2 = []\n",
    "    for sent1 , sent2 in zip(sent_list1 , sent_list2):\n",
    "        s1 = clean(sent1)\n",
    "        s2 = clean(sent2)\n",
    "        if(len(s1) > 0 and len(s2) > 0):\n",
    "            new_sent1.append(s1)\n",
    "            new_sent2.append(s2)\n",
    "    del sent_list1\n",
    "    del sent_list2\n",
    "    return new_sent1 , new_sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwqNoC9244ff"
   },
   "outputs": [],
   "source": [
    "english_sent, dutch_sent = clean_corpus(english_sent, dutch_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class defining a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4r2veetsBFb8"
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self , name , word2index , index2word):\n",
    "        self.name = name\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the words from each line and mapping word to index as well as index to word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKE8aqrQ5VMl"
   },
   "outputs": [],
   "source": [
    "def word_extractor(sent_list): \n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    index = 0\n",
    "    for sent in  sent_list:\n",
    "        for word in sent.split(\" \"):\n",
    "            if(word in word2index or word == \" \" or word == \"\"):\n",
    "                pass\n",
    "            else:\n",
    "                index2word[index] = word\n",
    "                word2index[word] = index\n",
    "                index = index + 1\n",
    "                \n",
    "    return word2index , index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDhDYgHcBYj0"
   },
   "outputs": [],
   "source": [
    "# e_word2index is the mapping from word to index for english language\n",
    "e_word2index , e_index2word = word_extractor(english_sent)\n",
    "d_word2index , d_index2word = word_extractor(dutch_sent)\n",
    "\n",
    "# Here we instatiate the languages which are dutch and english \n",
    "english_lang = Lang(\"English\" , e_word2index , e_index2word)\n",
    "dutch_lang = Lang(\"Dutch\" , d_word2index , d_index2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the probability matrix which stores the probabilities that words at rows and columns are matched to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZcQhQE17Mhw"
   },
   "outputs": [],
   "source": [
    "def probablity_init(lang1 , lang2):  # lang1 to lang2\n",
    "    total_words2 = len(lang2.word2index)\n",
    "    prob = 1.0/total_words2\n",
    "    prob_arr = np.full((len(lang1.word2index), len(lang2.word2index)), prob, dtype=float)\n",
    "    return prob_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_arr(S):\n",
    "    arr = S.split(\" \")\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Use to set which way to train and translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lang_1, lang_2 = dutch_lang, english_lang   #use this to train dutch to english\n",
    "#sents_1, sents_2 = dutch_sent, english_sent\n",
    "\n",
    "lang_1, lang_2 = english_lang, dutch_lang   #use this to train english to dutch\n",
    "sents_1, sents_2 = english_sent, dutch_sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Model 1 training - 1 iteration\n",
    "#### It takes as input the language lang1 which is to be translated into language lang2, which is the language to be translated in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_IBM1(lang1, lang2 , sents1 , sents2, prob_arr):   #lang1 to lang2\n",
    "\n",
    "    count_arr = np.zeros((len(lang1.word2index), len(lang2.word2index)), dtype=float)\n",
    "    total = np.zeros(len(lang1.word2index) , dtype = np.float32)\n",
    "    \n",
    "    for sent_index in range(len(sents1)):\n",
    "        s_total = np.zeros(len(lang2.word2index) , dtype = np.float32)\n",
    "        sents1_words = get_word_arr(sents1[sent_index])\n",
    "        sents2_words = get_word_arr(sents2[sent_index])\n",
    "        \n",
    "        for w2 in sents2_words:\n",
    "            w2_index = lang2.word2index[w2]\n",
    "            s_total[w2_index] = 0\n",
    "            for w1 in sents1_words:\n",
    "                w1_index = lang1.word2index[w1]\n",
    "                s_total[w2_index] += prob_arr[w1_index][w2_index]\n",
    "                \n",
    "        for w2 in sents2_words:\n",
    "            w2_index = lang2.word2index[w2]\n",
    "            for w1 in sents1_words:\n",
    "                w1_index = lang1.word2index[w1]\n",
    "                temp = prob_arr[w1_index][w2_index]/s_total[w2_index]\n",
    "                count_arr[w1_index][w2_index] += temp\n",
    "                total[w1_index] += temp\n",
    "                \n",
    "    break_loop = 1\n",
    "    result = 0\n",
    "    for w1 in lang1.word2index:\n",
    "        w1_index = lang1.word2index[w1]\n",
    "        for w2 in lang2.word2index:\n",
    "            w2_index = lang2.word2index[w2]\n",
    "            prev = prob_arr[w1_index][w2_index]\n",
    "            prob_arr[w1_index][w2_index] = count_arr[w1_index][w2_index]/total[w1_index]\n",
    "            new = prob_arr[w1_index][w2_index]\n",
    "            delta = (new - prev)**2\n",
    "            result  = result + delta\n",
    "            \n",
    "    if(result**0.5 < 0.5):\n",
    "        break_loop = 0 \n",
    "    print(result**0.5)\n",
    "    if(break_loop == 1):\n",
    "        redo = True\n",
    "    else:\n",
    "        redo = False\n",
    "        \n",
    "    return redo    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the probability array and call the train IBM1 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-4bf3247e4461>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#redo = train_IBM1_dutch_to_eng()    #use this to train dutch to english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#redo = train_IBM1_eng_to_dutch()   #use this to train english to dutch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mredo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_IBM1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msents_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msents_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-8550e9b3f010>\u001b[0m in \u001b[0;36mtrain_IBM1\u001b[1;34m(lang1, lang2, sents1, sents2, prob_arr)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0ms_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0msents1_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msents2_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "redo = True\n",
    "cnt = 0\n",
    "\n",
    "prob_arr = probablity_init(lang_1 , lang_2)    #use this to train english to dutch\n",
    "\n",
    "for i in range(10):     # while redo\n",
    "    start_time = time.time()\n",
    "    redo = train_IBM1(lang_1,lang_2, sents_1,sents_2, prob_arr)\n",
    "    cnt = cnt + 1\n",
    "    print(\"Iteration \"+str(cnt))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_map(lang1, lang2, prob_arr):   #makes a map between the lang1 word and the correspondingly predicted lang2 word\n",
    "    result ={}\n",
    "    for i in range(prob_arr.shape[0]):\n",
    "        maxi=-1\n",
    "        word1=lang1.index2word[i]\n",
    "        for j in range(prob_arr.shape[1]):\n",
    "            if(prob_arr[i][j]>maxi):\n",
    "                maxi = prob_arr[i][j]\n",
    "                word2 = lang2.index2word[j]\n",
    "        result[word1] = [word2, maxi]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Enter name of the model you want to save as IBM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt1\n"
     ]
    }
   ],
   "source": [
    "model_name_IBM1 = input()  #Enter name of the IBM1 model mapping to save\n",
    "result = make_result_map(lang_1, lang_2, prob_arr)\n",
    "save_dict(result_path, model_name_IBM1, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### IBM Model 2 training -1 iteration\n",
    " It takes as input the language lang1 which is to be translated into language lang2, which is the language to be translated in.\n",
    " Also, the maximum length of a sentence in lang1 and lang2 is given as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_IBM2(lang1, lang2 , sents1 , sents2, prob_arr, max_len1, max_len2):   #lang1 to lang2\n",
    "\n",
    "    count_arr = np.zeros((len(lang1.word2index), len(lang2.word2index)), dtype=float)\n",
    "    total = np.zeros(len(lang1.word2index) , dtype = np.float32)\n",
    "   \n",
    "    count_arr_a = np.zeros((max_len1, max_len2, max_len1, max_len2), dtype=float)\n",
    "    total_a = np.zeros((max_len2, max_len2, max_len1), dtype=float)\n",
    "   \n",
    "   \n",
    "    for sent_index in range(len(sents1)):\n",
    "        s_total = np.zeros(len(lang2.word2index) , dtype = np.float32)\n",
    "        sents1_words = get_word_arr(sents1[sent_index])\n",
    "        sents2_words = get_word_arr(sents2[sent_index])\n",
    "        l1 = len(sents1_words)\n",
    "        l2 = len(sents2_words)\n",
    "        j = 0\n",
    "        for w2 in sents2_words:\n",
    "            w2_index = lang2.word2index[w2]\n",
    "            s_total[w2_index] = 0\n",
    "            i = 0\n",
    "            for w1 in sents1_words:\n",
    "                w1_index = lang1.word2index[w1]\n",
    "                s_total[w2_index] += prob_arr[w1_index][w2_index] * a_mat[i][j][l1-1][l2-1]\n",
    "                i = i + 1\n",
    "            j = j + 1\n",
    "           \n",
    "        j = 0    \n",
    "        for w2 in sents2_words:\n",
    "            w2_index = lang2.word2index[w2]\n",
    "            i = 0\n",
    "            for w1 in sents1_words:\n",
    "                w1_index = lang1.word2index[w1]\n",
    "                temp = (prob_arr[w1_index][w2_index] * a_mat[i][j][l1-1][l2-1])/ s_total[w2_index]\n",
    "                count_arr[w1_index][w2_index] += temp\n",
    "                total[w1_index] += temp\n",
    "                count_arr_a[i][j][l1-1][l2-1] += temp\n",
    "                total_a[j][l2-1][l1-1] += temp\n",
    "                i = i + 1\n",
    "            j = j + 1\n",
    "               \n",
    "    break_loop = 1\n",
    "    result = 0\n",
    "    for w1 in lang1.word2index:\n",
    "        w1_index = lang1.word2index[w1]\n",
    "        for w2 in lang2.word2index:\n",
    "            w2_index = lang2.word2index[w2]\n",
    "            prev = prob_arr[w1_index][w2_index]\n",
    "            prob_arr[w1_index][w2_index] = count_arr[w1_index][w2_index]/total[w1_index]\n",
    "            new = prob_arr[w1_index][w2_index]\n",
    "            delta = (new - prev)**2\n",
    "            result  = result + delta\n",
    "   \n",
    "    for i in range(max_len1):\n",
    "        for  j in range(max_len2):\n",
    "            for l2 in range(max_len2):\n",
    "                for l1 in range(max_len1):\n",
    "                    if count_arr_a[i][j][l1][l2] == 0 :\n",
    "                        continue\n",
    "                    a_mat[i][j][l1][l2] = count_arr_a[i][j][l1][l2] / total_a[j][l2][l1]    \n",
    "   \n",
    "    if(result**0.5 < 0.5):\n",
    "        break_loop = 0\n",
    "        \n",
    "    print(result**0.5)\n",
    "\n",
    "    if(break_loop == 1):\n",
    "        redo = True\n",
    "    else:\n",
    "        redo = False\n",
    "       \n",
    "    return redo    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix a_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mat(lang1, lang2, max_len1, max_len2):\n",
    "    a_mat = np.zeros((max_len1, max_len2, max_len1, max_len2), dtype=float)\n",
    "    for l in range(max_len1):\n",
    "        prob = 1/(l+1)\n",
    "        a_mat[:,:,l,:] = prob\n",
    "    return a_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the probability array and call the train IBM2 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-85eae90b062a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#redo = train_IBM2_dutch_to_eng(max_l_dutch, max_l_eng)    #use this to train dutch to english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#redo = train_IBM2_eng_to_dutch(max_l_eng, max_l_dutch)   #use this to train english to dutch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mredo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_IBM2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msents_1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0msents_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5661048dd980>\u001b[0m in \u001b[0;36mtrain_IBM2\u001b[1;34m(lang1, lang2, sents1, sents2, prob_arr, max_len1, max_len2)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0ms_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0msents1_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msents2_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "max_l_1 = 0    #calculate maximum number of tokens in a sentence of foreign lang\n",
    "for sent in sents_1:\n",
    "    max_l_1=max(max_l_1, len(sent.split()))\n",
    "\n",
    "max_l_2 = 0   #calculate maximum number of tokens in a sentence of known lang\n",
    "for sent in sents_2:\n",
    "    max_l_2=max(max_l_2, len(sent.split()))\n",
    "\n",
    "redo = True\n",
    "cnt = 0\n",
    "\n",
    "a_mat = initialize_mat(lang_1, lang_2, max_l_1, max_l_2)\n",
    "prob_arr = probablity_init(lang_1, lang_2)\n",
    "\n",
    "for i in range(10):   # while redo\n",
    "    start_time = time.time()\n",
    "    redo = train_IBM2(lang_1, lang_2 , sents_1 , sents_2, prob_arr, max_l_1, max_l_2)\n",
    "    cnt = cnt + 1\n",
    "    print(\"Iteration \"+str(cnt))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter name of this model that you want to save as IBM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2d_2\n"
     ]
    }
   ],
   "source": [
    "model_name_IBM2 = input()   # Enter name of the IBM2 model mapping to save\n",
    "result = make_result_map(lang_1, lang_2, prob_arr)\n",
    "save_dict(result_path, model_name_IBM2, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2d_2\n"
     ]
    }
   ],
   "source": [
    "result_name = input()  #Enter the mapping name you want to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = load_dict(result_path, result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):  # lang1 to lang2\n",
    "    temp = sentence.lower().split(\" \")\n",
    "    out = []\n",
    "    for w in temp:\n",
    "        w = str(w)\n",
    "        regex = re.compile('[^1-9a-zA-Z ]')\n",
    "        w =  regex.sub(\"\" , w).strip().lower()\n",
    "        if(w == \" \" or w == \"\"):\n",
    "            continue\n",
    "        if(w in translations):\n",
    "            out.append(translations[w][0])\n",
    "        else:\n",
    "            out.append(w)\n",
    "    out = \" \".join(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intersection(lst1, lst2):  #intersection of 2 lists\n",
    "    return len(list(set(lst1) & set(lst2))) \n",
    "\n",
    "def Union(lst1, lst2):   #union of 2 lists\n",
    "    final_list = list(set().union(lst1, lst2)) \n",
    "    return final_list\n",
    "\n",
    "def clean_score(S):   #clean the sentence\n",
    "    S = S.split(\" \")\n",
    "    new_S = []\n",
    "    for w in S:\n",
    "        w = str(w)\n",
    "        regex = re.compile('[^1-9a-zA-Z ]')\n",
    "        w =  regex.sub(\"\" , w).strip().lower()\n",
    "        new_S.append(w)\n",
    "    S = \" \".join(new_S)\n",
    "    return S\n",
    "\n",
    "def cosine(doc1, doc2):\n",
    "    \n",
    "    words1 = clean_score(doc1.lower()).split()\n",
    "    words2 = clean_score(doc2.lower()).split()\n",
    "    allwords = Union(words1, words2)\n",
    "    freq1={}\n",
    "    for word in words1:\n",
    "        freq1[word] = 0\n",
    "\n",
    "    for word in words1:\n",
    "        freq1[word] += 1\n",
    "\n",
    "    freq2={}\n",
    "    for word in words2:\n",
    "        freq2[word] = 0\n",
    "\n",
    "    for word in words2:\n",
    "        freq2[word] += 1\n",
    "\n",
    "    wt1={}\n",
    "    for f in freq1:\n",
    "        wt1[f] = 1 + math.log10(freq1[f])\n",
    "\n",
    "    wt2={}\n",
    "    for f in freq2:\n",
    "        wt2[f] = 1 + math.log10(freq2[f])\n",
    "\n",
    "    vec1=[]\n",
    "    vec2=[]\n",
    "\n",
    "    vec1norm = 0\n",
    "    for word in allwords:\n",
    "        if word in wt1:\n",
    "            vec1.append(wt1[word])\n",
    "            vec1norm += wt1[word]*wt1[word]\n",
    "        else:\n",
    "            vec1.append(0)\n",
    "    vec1norm = math.sqrt(vec1norm)  \n",
    "    vec2norm = 0\n",
    "    for word in allwords:\n",
    "        \n",
    "        if word in wt2:\n",
    "            vec2.append(wt2[word])\n",
    "            vec2norm += wt2[word]*wt2[word]\n",
    "        else:\n",
    "            vec2.append(0)\n",
    "\n",
    "    vec2norm = math.sqrt(vec2norm)\n",
    "    cos = 0\n",
    "    for i in range(0,len(vec1)):\n",
    "        cos += vec1[i]*vec2[i]\n",
    "        \n",
    "    cos = cos / (vec1norm * vec2norm)\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Coefficent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(doc1, doc2):\n",
    "    words1 = clean_score(doc1.lower()).split()\n",
    "    words2 = clean_score(doc2.lower()).split()\n",
    "    \n",
    "    allwords = Union(words1, words2)\n",
    "    jacardCoeff = Intersection(words1,words2) / len(allwords)\n",
    "    return jacardCoeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give input the path of the document to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_data_english.txt\n",
      "./training_data_dutch.txt\n"
     ]
    }
   ],
   "source": [
    "test_data_path = input()\n",
    "correct_trans_path = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate the whole doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_training(lang1_path , lang2_path):   # lang1 to lang2\n",
    "    df_lang1 = pd.read_fwf(lang1_path , header = None)\n",
    "    df_lang2 = pd.read_fwf(lang2_path , header = None)\n",
    "    lang1_sents = df_lang1[0].values.tolist()\n",
    "    lang2_sents = df_lang2[0].values.tolist()\n",
    "    lang1_out = []\n",
    "    for lang1_sent in lang1_sents  :\n",
    "        lang1_out.append(translate(lang1_sent))\n",
    "    lang2_doc = \" \".join(lang2_sents)\n",
    "    lang1_doc = \" \".join(lang1_out)\n",
    "    \n",
    "    cos = cosine(lang1_doc,lang2_doc)\n",
    "    print(\"Cosine similarity of docs is \" + str(cos))\n",
    "    \n",
    "    jac = jaccard(lang1_doc, lang2_doc)\n",
    "    print(\"Jaccard coefficient of docs is \" + str(jac))\n",
    "    \n",
    "    arr = [cos, jac]\n",
    "    with open('resultdoc.txt', 'w', encoding=\"utf-8\") as fileh:\n",
    "        fileh.writelines(\"%s\\n\" % place for place in arr)\n",
    "    \n",
    "    return lang1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_test(lang1_path):   # lang1 to lang2\n",
    "    df_lang1 = pd.read_fwf(lang1_path , header = None)\n",
    "    lang1_sents = df_lang1[0].values.tolist()\n",
    "    lang1_out = []\n",
    "    for lang1_sent in lang1_sents  :\n",
    "        lang1_out.append(translate(lang1_sent))\n",
    "    return lang1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity of docs is 0.7139667190750636\n",
      "Jaccard coefficient of docs is 0.31899531668153436\n"
     ]
    }
   ],
   "source": [
    "output = translate_training(test_data_path, correct_trans_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save translated lines to 'output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output.txt', 'w', encoding=\"utf-8\") as filehandle:\n",
    "    filehandle.writelines(\"%s\\n\" % place for place in output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IR_assign.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
